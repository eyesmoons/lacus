# java home
java.home=/Users/casey/Data/install/jdk1.8.0_311
# hadoop config
hadoop.conf.dir=/Users/casey/Data/install/hadoop-3.3.1/etc/hadoop
# spark home
spark.client.home=/Users/casey/Data/install/spark-3.3.1-bin-hadoop3
# spark log path
spark.log.path=/Users/casey/Data/install/spark-3.3.1-bin-hadoop3/logs
# spark sql file dir
spark.sql.file.dir=/Users/casey/Data/install/spark-3.3.1-bin-hadoop3/data/files
# hdfs username
hadoop.username=casey
# defaultFS
hdfs.defaultFS=hdfs://hadoop1:9000
# if resource manager HA is disabled, keep this value empty
yarn.resourcemanager.ha.rm.ids=
# if resource manager HA is enabled or not use resource manager, please keep the default value;
# if resource manager is single, you only need to replace hadoop1 to actual resource manager hostname
yarn.application.status.address=http://hadoop1:%s/ws/v1/cluster/apps/%s
# job history status url
yarn.job.history.status.address=http://hadoop1:19888/ws/v1/history/mapreduce/jobs/%s
# yarn rest api address
yarn.restapi-address:http://hadoop2:8088/proxy/
# yarn node rest api address
yarn.node-address:http://hadoop2:8088/cluster/nodes
# yarn resource manager port
yarn.resource.manager.httpaddress.port=8088
# local temporal dir，used to store the temporary files
data.basedir.path=/Users/casey/tmp/data/
# 是否启用认证
hadoop.security.authentication.startup.state=false
java.security.krb5.conf.path=
login.user.keytab.path=
login.user.keytab.username=
# kafka servers
kafka.bootstrapServers=hadoop1:9092,hadoop2:9092,hadoop3:9092
# 数据采集配置文件在hdfs上的路径
flink.hdfs.collector.conf-path=/rtc/conf/
# 数据采集主jar包在hdfs上的路径
flink.hdfs.collector.job-jars-path=/rtc/engine/
# 数据采集主jar包名称
flink.hdfs.collector.jar-name=lacus-rtc-engine.jar
# 数据采集依赖包在hdfs上的路径
flink.hdfs.collector.lib-path=/rtc/libs
# 数据采集flink主jar包在hdfs上的路径
flink.hdfs.dist-jar-path=/rtc/libs/flink-dist-1.16.2.jar
# savepoint默认路径
flink.default.savepoint.path=hdfs://hadoop1:9000/flink/savepoint/lacus/
# 程序部署目录，其中flink sql任务和spark sql任务的jar包也需要放在这里
lacus.application.home=/Users/casey/Data/lacus/
# flink sql任务jar包
flink.sql.job.jar=lacus-flink-sql-app-2.0.0-jar-with-dependencies.jar
# spark sql app
spark.sql.job.jar=lacus-spark-sql-app-2.0.0-jar-with-dependencies.jar
# Local Cluster模式flink集群地址, docker访问本机网络：host.docker.internal
flink.rest.http.address=http://localhost:8081
# Standalone Cluster模式flink集群地址
flink.rest.ha.http.address=http://hadoop1:8081
# 本地flink客户端路径 /opt/software/flink1.16.3/
flink.client.home=/Users/casey/Data/install/flink-1.16.2
# flink任务执行目录，用于生产临时文件 /opt/software/lacus/execute/flink/
flink.job.execute.home=/Users/casey/Data/install/flink-1.16.2/temp/
